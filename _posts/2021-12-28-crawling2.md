---
title: "웹 크롤링 기본"
categories:
 - Computer
tags:
 - Web Crawling
last_modified_at: 2021-12-28T16:35:00-40:00
---

*Dave Lee 강사님의 "파이썬입문과 크롤링기초 부트캠프"의 강의 내용에 덧붙여 작성한 내용입니다.*

<br/>

<br/>

####  웹 크롤링을 어떻게 할 것인가? 

웹 크롤링을 하는 방법에는 여러 가지 방법이 있지만, 일단 다음과 같은 목표로 웹 크롤링을 하고자 한다. 

---

1. 파이썬을 이용하여
2. 데이터를 가져오고자 하는 웹 브라우저의 주소에서
3. html을 요청한 후 
4. 원하는 데이터만 추출하는 것

---

<br/>

<br/>

#### 파이썬 예제 

파이썬에는 웹 크롤링을 할 수 있도록 돕는 라이브러리를 여러 가지 제공하고 있다. 여기서는 정적 크롤링을 위해서는 BeautifulSoup, 동적 크롤링을 위해서는 Selenium을 다루고자 한다. 정적 크롤링과 동적 크롤링이 무엇인지는 후에 다루도록 한다. 파이썬의 라이브러리를 통해 웹 크롤링을 하기 때문에 코드를 작성하는 것은 그리 어렵지 않다. 다만 이 코드를 외우기보다는 어떤 흐름으로 코드를 작성하는지 아는 것이 포인트이다. 

```python
import requests 
from bs4 import BeautifulSoup
```

일단 **requests**를 왜 호출하는가는 **"웹 브라우저로부터 html을 요청하기 위함"**이다. **bs4** 패키지가 설치가 안 되었다면 Anaconda Prompt나 cmd 창에서 pip install bs4를 통해 설치해야 한다. 위에 불러오는 패키지 두 가지가 웹 크롤링에서 주요하게 사용되는 패키지이다. 

<br/>

위 라이브러리를 호출하는 게 완료되었다면 다음으로 해야할 것은 **웹 페이지에서 html 요청하기**이다. 예를 들어 Naver 메인 페이지의 html을 가져오고 싶다고 한다면 requests의 get 메서드를 이용하여 다음과 같이 불러올 수 있다. get 함수 안에는 인자로 원하는 웹페이지의 url을 넣어주면 된다. 

```python
res = requests.get("https://www.naver.com")
```

<br/>

우리가 어떤 웹페이지에 접속해서 해당 화면을 보는 것은 해당 웹페이지가 서버로부터 데이터를 요청해서 불러온 것을 보는 것이다. 이렇게 요청하면 서버에서 요청에 대한 회신을 숫자로 제공하는데 404, 200, ... 등이 예시이다. 따라서 우리도 웹페이지의 데이터를 요청한 것이기 때문에 요청에 대한 회신을 확인할 수 있는데 res라고 지정한 객체의 status_code 속성을 통해 확인할 수 있다. 

```python
res.status_code
```

위의 결과가 200이라면 데이터를 잘 가져온 것이고, 그 외의 결과에는 숫자에 따라서 다양한 의미가 있지만 뭔가 문제가 발생했다고 생각하면 된다. 







데이터를 불러오는데 성공했다면 res.content로 어떤 데이터들이 있는지 확인해보면 알 수 없이 복잡한 코드들로 구성되어 있음을 확인할 수 있다. 이렇게 둔 상태로 원하는 데이터를 추출하는 것은 어려우니 **파싱(Parsing)**이라는 작업을 거친다. **파싱**이란 **문자열의 의미를 분석하는 것**을 말하는데 알아보기 어려운 데이터를 BeautifulSoup에서 제공하는 parser를 이용해 데이터를 깔끔하게 정리할 수 있다. 

```python
soup = BeautifulSoup(res.content, 'html.parser')
```

위 코드의 내용은 네이버로부터 받은 데이터를 html.parser를 통해서 파싱한 뒤에 soup 객체로 저장하라는 뜻이다. **이렇게 파싱해두면 soup 객체 안에서 원하는 데이터를 쉽게 찾아낼 수 있다!** 

<br/>

<br/>

이상으로 과정 1, 2, 3이 마무리되었다고 할 수 있다. 4번의 원하는 데이터만 추출하는 작업은 다음 예제를 통해서 다뤄보고자 한다. 







